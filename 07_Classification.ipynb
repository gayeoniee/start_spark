{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3c40c4-c982-4111-910f-2372af043e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8f96af75fc7f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>classificationExample1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9a04db4150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName('classificationExample1').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e1950e-6024-4c9c-a6f4-9f9276f653c7",
   "metadata": {},
   "source": [
    "# 타이타닉 데이터를 이용한 생존 여부\n",
    "## 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661a6ff3-65e1-4603-8960-3b2b723001fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|Gender| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 로드\n",
    "data = spark.read.csv('learning_spark_data/titanic.csv', header=True, inferSchema=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62dd66ba-6329-407a-b8ab-ed50c1244104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+------+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Gender|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+------+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|     0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+------+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, when, isnan\n",
    "# 결측치 처리\n",
    "null_counts = data.select(\n",
    "    [\n",
    "        sum(when(col(c).isNull() | isnan(c), 1).otherwise(0)).alias(c)\n",
    "        for c in data.columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86d85cde-c15c-4f41-a02f-a3f8ba87270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+\n",
      "|Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|\n",
      "+--------+------+------+----+-----+-----+-------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|\n",
      "+--------+------+------+----+-----+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "data_1 = data.select('Survived', 'Pclass', 'Gender', 'Age', 'SibSp', 'Parch', 'Fare')\n",
    "data_1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0e1a66b-a084-4bf3-85ea-e582f30129c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Age 결측치 처리 - 중앙값으로 처리\n",
    "median_age = data_1.approxQuantile(\"Age\", [0.5], 0.01)[0]\n",
    "median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806ddbb7-b11c-4869-abc9-e1f5956988a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+\n",
      "|Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|\n",
      "+--------+------+------+----+-----+-----+-------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|\n",
      "|       0|     3|  male|28.0|    0|    0| 8.4583|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|\n",
      "+--------+------+------+----+-----+-----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_1 = data_1.fillna({'Age':median_age})\n",
    "data_1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c59a1ef-d631-4408-8f58-8c757fbf6c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+----------+\n",
      "|Survived|Pclass|Gender| Age|SibSp|Parch|   Fare|SexIndexer|\n",
      "+--------+------+------+----+-----+-----+-------+----------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       0.0|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       1.0|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       1.0|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       1.0|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       0.0|\n",
      "+--------+------+------+----+-----+-----+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 인코딩 StringIndexer\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "indexer = StringIndexer(inputCol='Gender', outputCol='SexIndexer')\n",
    "data_1 = indexer.fit(data_1).transform(data_1)\n",
    "data_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d31cff7c-66a9-42d7-9620-279573a63b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|            features|Survived|\n",
      "+--------------------+--------+\n",
      "|[3.0,0.0,22.0,1.0...|       0|\n",
      "|[1.0,1.0,38.0,1.0...|       1|\n",
      "|[3.0,1.0,26.0,0.0...|       1|\n",
      "|[1.0,1.0,35.0,1.0...|       1|\n",
      "|[3.0,0.0,35.0,0.0...|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FeatureVector 생성\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Pclass', 'SexIndexer', 'Age', 'SibSp', 'Parch', 'Fare'],\n",
    "    outputCol='features'\n",
    ")\n",
    "data_1 = assembler.transform(data_1)\n",
    "data_1.select('features', 'Survived').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35d4ead4-8b29-4ce0-bb5c-d7674dc1d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "train_data, test_data = data_1.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b1cd2bc-2150-42e0-9234-3b0912bd93e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|            features|Survived|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|[1.0,1.0,50.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,21.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,24.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,28.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,28.0,0.0...|       0|       1.0|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Survived')\n",
    "lr_model = lr.fit(train_data)\n",
    "predict = lr_model.transform(test_data)\n",
    "predict.select('features', 'Survived', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a55a4842-9ee1-4ff1-89ce-87a8be15cd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "comp = predict.withColumn('correct', expr('case when Survived = prediction then 1 else 0 end'))\n",
    "comp.where('correct=0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a71758bf-a12a-46a7-a46b-4e7a4602440a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|            features|Survived|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|[1.0,1.0,50.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,21.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,24.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,28.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,28.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,28.0,0.0...|       0|       1.0|\n",
      "|[1.0,0.0,29.0,0.0...|       0|       1.0|\n",
      "|[3.0,1.0,9.0,1.0,...|       0|       1.0|\n",
      "|[3.0,1.0,14.0,0.0...|       0|       1.0|\n",
      "|[3.0,1.0,22.0,0.0...|       0|       1.0|\n",
      "|[3.0,1.0,28.0,0.0...|       0|       1.0|\n",
      "|[3.0,1.0,28.0,1.0...|       0|       1.0|\n",
      "|[1.0,0.0,40.0,0.0...|       1|       0.0|\n",
      "|[1.0,0.0,42.0,0.0...|       1|       0.0|\n",
      "|[1.0,0.0,45.0,0.0...|       1|       0.0|\n",
      "|[2.0,0.0,2.0,1.0,...|       1|       0.0|\n",
      "|[3.0,1.0,33.0,3.0...|       1|       0.0|\n",
      "|[3.0,1.0,38.0,1.0...|       1|       0.0|\n",
      "|[3.0,1.0,63.0,0.0...|       1|       0.0|\n",
      "|[3.0,0.0,6.0,0.0,...|       1|       0.0|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 틀린 데이터만 필터링\n",
    "predict.filter(col('Survived') != col('prediction')).select('features', 'Survived', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac9d7ec9-2d42-416f-9229-c13ebc40c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8137931034482758"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도 평가\n",
    "comp.selectExpr('avg(correct) as accuracy').collect()[0]['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef86e0ab-f82b-4477-b278-7623d88958d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.865632318501171"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = BinaryClassificationEvaluator(\n",
    "    labelCol='Survived',\n",
    "    rawPredictionCol='rawPrediction',\n",
    "    metricName='areaUnderROC'\n",
    ")\n",
    "\n",
    "auc = eval.evaluate(predict)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589da50-c875-48a3-aa6d-4f2a38defd34",
   "metadata": {},
   "source": [
    "AUROC -> X축 FPR, y축 TPR의 곡선 아래 면적 => 1에 가까울 수록 성능 good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1bf977a-c439-4260-bdab-d67d7e0abecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c810b7e4-ff6a-4a9d-a8f3-c6b34dbf1bdd",
   "metadata": {},
   "source": [
    "# libsvm 파일 형식의 처리\n",
    "텍스트 파일 형식, 희소데이터용 압축 파일 -> 메모리, 처리속도 개선 => 머신러닝에서 사용\n",
    "\n",
    "레이블 행:값 행:값 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ab4d261-fe64-4b9f-a792-b0676774e74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8f96af75fc7f:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>classificationExample2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f99cfda5d50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('classificationExample2').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "877ba27e-db68-4190-a1fb-7542d96203fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = spark.read.format('libsvm').load('learning_spark_data/sample_libsvm_data.txt')\n",
    "data2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7c9af50-cc6e-4ab8-b5e2-39bc9dadefbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2146fb7f-ffae-45af-a7b2-9c5b4803d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data2.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4866db6-fa94-4413-a49b-29f39ca1e9e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 35)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64c54eaf-725d-4f4e-ae1b-a4f65c983b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[100,101,102...|[0.61110663774844...|[0.64819319976427...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[1.88762878149153...|[0.86848492926088...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[1.48203827311578...|[0.81488025243698...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[1.32973716405211...|[0.79079715531589...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[1.14364775038872...|[0.75834874070092...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.2, elasticNetParam=0.7)\n",
    "lrmodel = lr.fit(train_data)\n",
    "pred = lrmodel.transform(test_data)\n",
    "pred.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dfe5931-4cfe-422f-b1a3-6a1b094621a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = BinaryClassificationEvaluator(\n",
    "    metricName='areaUnderROC'\n",
    ")\n",
    "\n",
    "auc = eval.evaluate(pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af1a9111-3b12-4d33-b3b2-99e2dc4eefab",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb6ef5c-5721-421d-8a68-8e03e8690486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
